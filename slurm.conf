# slurm.conf
#
# See the slurm.conf man page for more information.
#
ClusterName=nsls2
ControlMachine=slurmctld
ControlAddr=slurmctld
#BackupController=
#BackupAddr=
#
SlurmUser=slurm
#SlurmdUser=root
SlurmctldPort=6817
SlurmdPort=6818
AuthType=auth/munge
#JobCredentialPrivateKey=
#JobCredentialPublicCertificate=
StateSaveLocation=/var/lib/slurmd
SlurmdSpoolDir=/var/spool/slurmd
SwitchType=switch/none

# MpiDefault=none
MpiDefault               = pmi2

SrunPortRange            = 60001-63000

TopologyPlugin           = topology/tree

PriorityType             = priority/multifactor

PriorityDecayHalfLife    = 60-00:00:00
PriorityCalcPeriod       = 00:05:00
PriorityFavorSmall       = 0
PriorityMaxAge           = 1-00:00:00
PriorityUsageResetPeriod = NONE
PriorityWeightAge        = 184320
PriorityWeightFairShare  = 129600
PriorityWeightJobSize    = 0
PriorityWeightPartition  = 184340
PriorityWeightQOS        = 253440

PropagateResourceLimitsExcept = CPU,AS

UnkillableStepTimeout=600
ResumeTimeout=600


SlurmctldPidFile=/var/run/slurmd/slurmctld.pid
SlurmdPidFile=/var/run/slurmd/slurmd.pid
# ProctrackType=proctrack/linuxproc
ProctrackType=proctrack/cgroup
#PluginDir=
#CacheGroups=0
#FirstJobId=
ReturnToService=0
#MaxJobCount=
#PlugStackConfig=
#PropagatePrioProcess=
#PropagateResourceLimits=
#PropagateResourceLimitsExcept=
#Prolog=
#Epilog=
#SrunProlog=
#SrunEpilog=
#TaskProlog=
#TaskEpilog=

# TaskPlugin=task/cgroup
TaskPlugin               = task/affinity,task/cgroup

#TrackWCKey=no
#TreeWidth=50
#TmpFS=
#UsePAM=
#
# TIMERS
SlurmctldTimeout=300
SlurmdTimeout=300
InactiveLimit=0
MinJobAge=300
KillWait=30
Waittime=0
#
#
# SCHEDULING
#
SchedulerType            = sched/backfill
SelectType               = select/cons_tres

# SelectTypeParameters     = CR_Core_Memory
SelectTypeParameters     = CR_Core

DefMemPerCPU             = 16000
#PreemptType              = preempt/qos
PreemptMode              = off
PrologFlags              = Alloc,Contain,X11
# PrologFlags              = Alloc,Contain
SchedulerParameters      = bf_max_job_test=500,default_queue_depth=500,bf_window=2880
# DisableRootJobs          = yes

#
# LOGGING
SlurmctldDebug=3
SlurmctldLogFile=/var/log/slurm/slurmctld.log
SlurmdDebug=3
SlurmdLogFile=/var/log/slurm/slurmd.log
JobCompType=jobcomp/filetxt
JobCompLoc=/var/log/slurm/jobcomp.log
#
# ACCOUNTING
JobAcctGatherType=jobacct_gather/linux
# JobacctGatherType=jobacct_gather/cgroup
JobAcctGatherFrequency=30
#
AccountingStorageType=accounting_storage/slurmdbd

AccountingStoreFlags      = job_comment
AccountingStorageTRES     = cpu,mem,gres/gpu,gres/gpu:volta
AccountingStorageEnforce  = associations,limits,qos,safe


AccountingStorageHost=slurmdbd
AccountingStoragePort=6819
#AccountingStorageLoc=slurm_acct_db
#AccountingStoragePass=
#AccountingStorageUser=
#

#
# GRES
#
GresTypes                 = gpu
RebootProgram             = /usr/sbin/reboot


# COMPUTE NODES
# NodeName=c[1-2] RealMemory=1000 State=UNKNOWN
NodeName=saturn4 RealMemory=1000 State=UNKNOWN

#
# PARTITIONS
# PartitionName=normal Default=yes Nodes=c[1-2] Priority=50 DefMemPerCPU=500 Shared=NO MaxNodes=2 MaxTime=5-00:00:00 DefaultTime=5-00:00:00 State=UP

# sdcc partition defs
# PartitionName=jupyter-staff-chx      OverSubscribe=FORCE:50 State=UP Nodes=saturn4             DefaultTime=00-00:10:00 qos=part_jupyter AllowQOS=ALL Priority=10000  Default=no

# PartitionName=jupyter-staff-chx Default=yes Nodes=saturn4 Priority=50 DefMemPerCPU=500 Shared=NO MaxNodes=2 MaxTime=5-00:00:00 DefaultTime=5-00:00:00 State=UP
PartitionName=jupyter-staff-chx      OverSubscribe=FORCE:50 State=UP Nodes=saturn4             DefaultTime=00-00:10:00 AllowQOS=ALL Priority=10000  Default=no