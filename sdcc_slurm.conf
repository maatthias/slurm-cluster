# ***************************************************************************
# * WARNING -- THIS FILE IS PUPPET MANAGED !! -- CHANGES WILL BE OVERWRITTEN
# *
# * slurm.conf Config File
# *
# ***************************************************************************

SlurmctldHost            = master1
SlurmctldHost            = master2

#MailProg=/bin/mail
MpiDefault               = pmi2
#MpiParams=ports=#-#
ProctrackType            = proctrack/cgroup
ReturnToService          = 1
SlurmctldPidFile         = /var/run/slurmctld.pid
#SlurmctldPort=6817
SlurmdPidFile            = /var/run/slurmd.pid
#SlurmdPort=6818
SrunPortRange            = 60001-63000
SlurmdSpoolDir           = /var/spool/slurmd
SlurmUser                = slurm
#SlurmdUser=root
#StateSaveLocation        = /var/spool/slurmctld/
StateSaveLocation        = /mnt/slurm_state/
SwitchType               = switch/none
TaskPlugin               = task/affinity,task/cgroup
#JobSubmitPlugins         = lua
TopologyPlugin           = topology/tree

#
#PRIORITY
#
PriorityType             = priority/multifactor
# try huge half life (1 year) in combination with huge weightfactor(3 months)
# (this is to combat the decay of allocation-hard-timelimits via fairshare exp decrease)
PriorityDecayHalfLife    = 60-00:00:00
PriorityCalcPeriod       = 00:05:00
PriorityFavorSmall       = 0
PriorityMaxAge           = 1-00:00:00
PriorityUsageResetPeriod = NONE
PriorityWeightAge        = 184320
PriorityWeightFairShare  = 129600
PriorityWeightJobSize    = 0
PriorityWeightPartition  = 184340
PriorityWeightQOS        = 253440

PropagateResourceLimitsExcept = CPU,AS

#
# TIMERS
#KillWait=30
#MinJobAge=300
#SlurmctldTimeout=120
#SlurmdTimeout=300
#SlurmctldParameters=enable_configless
UnkillableStepTimeout=600
ResumeTimeout=600

#
# SCHEDULING
#
SchedulerType            = sched/backfill
SelectType               = select/cons_tres
SelectTypeParameters     = CR_Core
DefMemPerCPU             = 16000
#PreemptType              = preempt/qos
PreemptMode              = off
PrologFlags              = Alloc,Contain,X11
SchedulerParameters      = bf_max_job_test=500,default_queue_depth=500,bf_window=2880
DisableRootJobs          = yes

#
# LOGGING AND ACCOUNTING
#
AccountingStorageType     = accounting_storage/slurmdbd
AccountingStoreFlags      = job_comment
AccountingStorageHost     = master1
AccountingStorageTRES     = cpu,mem,gres/gpu,gres/gpu:volta
#AccountingStorageLoc=
ClusterName               = nsls2
AccountingStorageEnforce  = associations,limits,qos,safe
#JobAcctGatherFrequency=30
JobAcctGatherType         = jobacct_gather/linux
#SlurmctldDebug=3
SlurmctldLogFile          = /var/log/slurm/slurmctld.log
#SlurmdDebug=3
SlurmdLogFile             = /var/log/slurm/slurmd.log

#
# EPILOG and PROLOG
#

#Prolog                    = /hpcgpfs01/software/costin/slurm-prolog.sh
#TaskProlog                = /nsls2/software/slurm/slurm-taskprolog.sh
#Epilog                    = /hpcgpfs01/software/costin/nbody-epilog.sh

#
# GRES
#
GresTypes                 = gpu
RebootProgram             = /usr/sbin/reboot

#salloc replacement
LaunchParameters=use_interactive_step
InteractiveStepOptions="--interactive --pty --preserve-env --mpi=none $SHELL"

#
# COMPUTE NODES
#
#debug queue: jobs run for a maximum of 30 minutes
#long queue: jobs run for a maximum of 24 hours 

NodeName  = hpc[001-014] NodeAddr=hpc[001-014]    CPUs=48  SocketsPerBoard=2 CoresPerSocket=24 ThreadsPerCore=1 RealMemory=771487 Feature=ib State=UNKNOWN Weight=100
NodeName  = hpc[015-019] NodeAddr=hpc[015-019]    CPUs=48  SocketsPerBoard=2 CoresPerSocket=24 ThreadsPerCore=1 RealMemory=771487 Feature=ib State=UNKNOWN Weight=1
NodeName  = submit[1-2]  NodeAddr=submit[1-2]     CPUs=48  SocketsPerBoard=2 CoresPerSocket=24 ThreadsPerCore=1 RealMemory=771487 Feature=ib State=UNKNOWN
NodeName  = master[1-2]  NodeAddr=master[1-2]     CPUs=48  SocketsPerBoard=2 CoresPerSocket=24 ThreadsPerCore=1 RealMemory=771487 Feature=ib State=UNKNOWN
NodeName  = gpu[001-013] NodeAddr=gpu[001-013]    Gres=gpu:volta:2 CPUs=48 SocketsPerBoard=2 CoresPerSocket=24 ThreadsPerCore=1 RealMemory=771487 Feature=ib,volta State=UNKNOWN Weight=1000
NodeName  = xf11id-srv2                           CPUs=56  SocketsPerBoard=2 CoresPerSocket=14 ThreadsPerCore=2 RealMemory=771487            State=UNKNOWN
NodeName  = xf11id-srv3                           CPUs=88  SocketsPerBoard=2 CoresPerSocket=22 ThreadsPerCore=2 RealMemory=721487            State=UNKNOWN
NodeName  = saturn4                               CPUs=256 SocketsPerBoard=2 CoresPerSocket=64 ThreadsPerCore=2 RealMemory=2043441            State=UNKNOWN

PartitionName=debug    OverSubscribe=FORCE    State=UP Nodes=gpu002                            DefaultTime=00-00:10:00 qos=part_debug   AllowQOS=ALL Priority=10000  Default=no
PartitionName=normal   Shared=NO              State=UP Nodes=hpc[001,006-019],gpu[001,003-011,013] DefaultTime=00-00:10:00 qos=part_normal  AllowQOS=normal,long,high,debug Priority=10000  Default=yes
PartitionName=eic      Shared=NO              State=UP Nodes=hpc[015-019],gpu[003-008]         DefaultTime=00-00:10:00 qos=part_eic     AllowQOS=eic_normal,eic_long,high,debug Priority=1000   Default=no
PartitionName=jupyter  OverSubscribe=FORCE    State=UP Nodes=hpc[002-005],gpu012               DefaultTime=00-00:10:00 qos=part_jupyter AllowQOS=ALL Priority=10000  Default=no
PartitionName=chx      OverSubscribe=FORCE:50 State=UP Nodes=xf11id-srv[2-3]                   DefaultTime=00-00:10:00 qos=part_jupyter AllowQOS=ALL Priority=10000  Default=no
PartitionName=jupyter-staff-chx      OverSubscribe=FORCE:50 State=UP Nodes=saturn4             DefaultTime=00-00:10:00 qos=part_jupyter AllowQOS=ALL Priority=10000  Default=no
